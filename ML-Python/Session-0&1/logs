kaushiksk : <@U56N7T2F8|kaushiksk> has joined the channel

ag_1709 : <@U58DY3T7A|ag_1709> has joined the channel

mohitreddy1996 : <@U59U2PEK0|mohitreddy1996> has joined the channel

mahirjain25 : <@U5862FK17|mahirjain25> has joined the channel

ayush.work113 : <@U59H047SS|ayush.work113> has joined the channel

piyush : <@U58MG330T|piyush> has joined the channel

arvindsaik : <@U58GN7DDX|arvindsaik> has joined the channel

darshandv10 : <@U58337ESC|darshandv10> has joined the channel

samyak : <@U584D9FT2|samyak> has joined the channel

svarshini : <@U58R2AR8C|svarshini> has joined the channel

gtmb : <@U59H7U7C6|gtmb> has joined the channel

shashankp : <@U58NYR47L|shashankp> has joined the channel

anmol.jindal13 : <@U586X7GLR|anmol.jindal13> has joined the channel

gurupunskill : <@U5872KL73|gurupunskill> has joined the channel

decisiontreehugger : <@U58R173NY|decisiontreehugger> has joined the channel

npsnayak : <@U59J595EJ|npsnayak> has joined the channel

sanathk : <@U59JHRLAJ|sanathk> has joined the channel

harsha_1810 : <@U59HPU5JA|harsha_1810> has joined the channel

adithyabhatkajake : <@U5941NQQ2|adithyabhatkajake> has joined the channel

sony : <@U585LE3UZ|sony> has joined the channel

swapnil8424 : <@U598HEB6Y|swapnil8424> has joined the channel

kaushiksk : <@U56N7T2F8|kaushiksk> set the channel purpose: This channel is where all the sessions will be held.

kaushiksk : Hey guys!

kaushiksk : How many here ?

ag_1709 : :hand: 

ayush.work113 : :+1:

samyak : :+1: 

gurupunskill : :+1:

svarshini : :+1: 

kaushiksk : Ok. Hello guys and welcome to Practical tools for ML with Python. Let's hope we all learn a lot this summer :smile:

kaushiksk : 5/17 here. Let's wait till we have a 50% show of hands. We'll begin at 6:45pm.

gurupunskill : Cool :+1:

anmol.jindal13 : :+1: 

harsha_1810 : :+1:

piyush : :+1:

kaushiksk : Anyone else just joined in ?

kaushiksk : 5 more minutes and we'll start.

kaushiksk : I'll just go through some obligatory basics. You can choose to ignore them if you wish. We'll consider some things in detail as and when we encounter them later.

kaushiksk : I'll focus more on getting everyone on the same foot wrt familiarity with python

gurupunskill : Will we be applying python in jupyter or like normally?

kaushiksk : For today and today's assignment, normally. Everyone hasn't yet sent us their notebooks so we'll wait for that.

kaushiksk : We'll cover simple things anyway.

decisiontreehugger : <@U5872KL73> Notebooks make it much easier when dealing with data and definitely will increase your productivity. So mostly Jupyter.

kaushiksk : Yeah, as we move on Jupyter IS the way to go. So get familiar. I'll send some links anyway. But its just so much fun exploring on your own :stuck_out_tongue: 
Today we'll stick to the python interpreter and writing simply scripts.

kaushiksk : Ok.

kaushiksk : 6:46pm

kaushiksk : We'll begin

kaushiksk : Final show of hands everyone. One more time.

anmol.jindal13 : :+1: 

samyak : :+1: 

harsha_1810 : yeah :hand:

piyush : :+1: 

ayush.work113 : :hand:

gurupunskill : :+1:

shashankp : :+1: 

ag_1709 : :hand: 

kaushiksk : Alright. We'll kick off our summer project with a glorious 8/17 attendance.

kaushiksk : Once again, Welcome to Practical tools for ML with Python

kaushiksk : I'm Kaushik and today we'll justbrush up basic stats and write some code in python!

kaushiksk : Those who are in your PCs, fire up the terminal and open the python interpreter

kaushiksk : by typing python

kaushiksk : Or if you have ipython installed type ipython

kaushiksk : Windows users open cmd and type python

kaushiksk : or just chill. That's just so you can type in some stuff and see how cool python is for yoursellf.

svarshini : :hand: 

kaushiksk : 9/17 Awesome!

gurupunskill : kaushiksk: 50% woohoo

kaushiksk : Ok so most of you would've already read about the basic stats i'll cover today but i'll do it anyway because i'll use that to get you familiar with python scripting ?

kaushiksk : Ok. So before we get into ML and all its glory, we need to be able to look at any random dataset and be able to make some sort of conclusions right ?

kaushiksk : We'll do that today

kaushiksk : We'll need three things.

kaushiksk : 1. Data set
2. Tools
3. Ideas

kaushiksk : Data set : 
data = [255, 168, 125, 11102, 59, 120, 500, 2, 98, 25]

kaushiksk : That's from a nice little website i was reading through. I'll link it later. I'm using the same data to show you the proof of concept.

kaushiksk : You can consider those as response times for requests made on a server.

kaushiksk : Cool ?

gurupunskill : Okay

ag_1709 : :+1: 

harsha_1810 : yea fine

anmol.jindal13 : :+1: 

kaushiksk : Also take note, that's how a python list is represented

shashankp : :+1: 

kaushiksk : Now we'll dive right in. data is a variable name that represents our data here.

kaushiksk : In python terms that is what is called a list

kaushiksk : comma seperated values inside square brackets.

kaushiksk : More on lists later.

kaushiksk : Everyone with me ?

svarshini : Yes!

ayush.work113 : yup

harsha_1810 : yep

ag_1709 : Yup

gurupunskill : It's basically an array yes?

kaushiksk : You can think of it as an array, but we'll know it behaves differently in a while :slightly_smiling_face:

ag_1709 : Lists cannot be 2d, right?

kaushiksk : Ok Python later, stats first.

kaushiksk : So, before we do anything. Obligatory detour.

kaushiksk : Data.

kaushiksk : Lets just talk about data for a second.

kaushiksk : Ok ?

ag_1709 : :+1: 

kaushiksk : <@U56N7T2F8|kaushiksk> shared a file: <https://mlieeesmp.slack.com/files/kaushiksk/F59NXAT9A/img_20170506_193707.jpg|IMG_20170506_193707.jpg>

kaushiksk : there's two broad classifications. Qualitative (cue: Quality) and Quantitative(cue: Quantity)

kaushiksk : (Any comments on my handwriting may be withheld)

decisiontreehugger : You can put anything you want inside a list so you can even have things like a list of lists or even a list containing numbers, other lists and strings

ag_1709 : decisiontreehugger: Got it!

kaushiksk : Qualitative as the name suggests, describes Quality. This is not any sort of measured data.

kaushiksk : Two types:

kaushiksk : Nominal

kaushiksk : Here. You just have categories. You can number them any way you want. The numbers dont have any significance

kaushiksk : E.g

kaushiksk : Male - 0
Female - 1'

kaushiksk : Doesnt mean Female is better than male

kaushiksk : or anything like that

kaushiksk : The other is Ordinal

kaushiksk : Movie ratings

kaushiksk : 5 - EPICCCCCC
4 - Very good
3 - Good
2- Average
1 - Bad
0 - Dont even

kaushiksk : You know that 5 is better than 4

kaushiksk : 4 is better than 3

kaushiksk : This is an improvement from Nominal

kaushiksk : But then

kaushiksk : How better is 5 than 4 ?

kaushiksk : There's no way to say

kaushiksk : Because these are not any kinds of concrete measurements

kaushiksk : Everyone still with me ?

svarshini : Yeah

samyak : Yup

ag_1709 : :+1: 

ayush.work113 : kaushiksk: yes

harsha_1810 : :+1:

shashankp : :+1: 

kaushiksk : Individual messages can be reacted upon. Like facebook posts.

kaushiksk : So to reduce clutter like <@U5872KL73> suggest just react on my message !

kaushiksk : Ok moving on.

anmol.jindal13 : kaushiksk: So you defined them right?  Assigned a number to a quality

gurupunskill : And messages can have replies, like comments.

kaushiksk : We come to Quantitative Data.

kaushiksk : Yup. For someone else 5 can mean something else. That's why it's not reliable.

kaushiksk : Now quantitative data can be discrete or sontinuous

gurupunskill : Yeah but can't that be said for eny and every unit ?

kaushiksk : *continuous

kaushiksk : discrete - specific integer values

kaushiksk : say age in years, temperature in celcius

kaushiksk : continous - age

gurupunskill : Like 1kg can mean something to someone else, It's more about how we defined and let the other person know yeah?

kaushiksk : 5 years 10 months 4 days 22 hours 3 mins 4 seconds

kaushiksk : ...

kaushiksk : you can go on

kaushiksk : basically some data that can take any value within a range

kaushiksk : Still with me ?

kaushiksk : Because now we get to do something interesting

kaushiksk : Let me grab the data again.

kaushiksk : data = [255, 168, 125, 11102, 59, 120, 500, 2, 98, 25]

kaushiksk : Now we'll cover Measures of Dispersion and talk a bit about Histograms.

kaushiksk : Ok so I give you that data. What do you do with it? How do you get a birds eye view ?

kaushiksk : Anybody know what are the measures of dispersion ?

ag_1709 : kaushiksk: Mean median mode

kaushiksk : Awesome

ag_1709 : kaushiksk: Standard deviation also maybe

kaushiksk : You all know it: Mean, Median Mode

gurupunskill : Variance as well

kaushiksk : Its just a fancy term to describe them. Measures of Dispersion. Makes you sound cool :stuck_out_tongue:

kaushiksk : That's measure of Central Tendency. We'll leave that for later.

samyak : isnt it only std deviation and variance as mean mode median is called measure of central tendency

kaushiksk : Uh. My bad. Mixed it up :smile:

kaushiksk : . Thanks for pointing that out <@U584D9FT2>

kaushiksk : #facepalm

gurupunskill : isskay xD

kaushiksk : Right. Measures of Central Tendency.

kaushiksk : Pardon the mistake.

kaushiksk : Ok so for our data we'll find the Mean median and mode

kaushiksk : data = [255, 168, 125, 11102, 59, 120, 500, 2, 98, 25]

kaushiksk : Mean: 1245.40
Median: 122.5

kaushiksk : Mode: anything. We only have unique values.

kaushiksk : Mean : Avg of all values.
Median : Middlemost value in the data when it is sorted.
Mode: Most repeating term.

kaushiksk : If we have even number of data points we just take the avg of the two middle values for median, which is what we've done here.

kaushiksk : Anybody want to tell me for what kind of data mode is a good way to convey some meaning ?

kaushiksk : Anybody wants to comment on this? What do you observe ?

gurupunskill : It implies the list is varied I guess?

gurupunskill : Since the median is much far from the mode or something?

kaushiksk : Wouldn't we use Variance or stddev for that ?

kaushiksk : Takers ?

ag_1709 : All the more, you have multiple modes, you cannot comment

kaushiksk : Ok we'll look at the histogram for our data. Which is taken from the same site.

kaushiksk : <https://cldup.com/CLt8N20YIH.png>

kaushiksk : Now, can someone make a comment about the mean and median ?

shashankp : kaushiksk: Because there is an extreme value 11102?

kaushiksk : Wrong thread ? :stuck_out_tongue:

kaushiksk : Mode is a good indicator for Qualitative data, where you use numbers as indicators. Large number of 5s just tells you people liked the movie.

arvindsaik : <@U5872KL73> try 
100,100,100,5,5,100,100,100
median and mean are far apart but dont vary much

shashankp : kaushiksk: There is an extreme value 11102 which shifts the mean towards greater side

kaushiksk : Awesome!

ag_1709 : Maybe the extent to which the data is scattered...

gurupunskill : <@U58GN7DDX> Yeah I realized

gurupunskill : <@U58DY3T7A> see <@U58GN7DDX> reply in the other thread

kaushiksk : Right <@U58NYR47L> is right. (check thread)

gurupunskill : You can't judge the variance of  data set with just the mean and median

kaushiksk : There is an outlier(a value that is much much different from all other values)

kaushiksk : And Mean is giving weightage to that

ag_1709 : Ok

kaushiksk : while Median doesnt care about outliers

kaushiksk : So mean is biased towards outliers, median is not.

kaushiksk : this is not good because outliers are data points we'd genrally want to discard.

kaushiksk : We clear till here ?

kaushiksk : Bring up doubts if you have right now. We'll move to scripting in a while

anmol.jindal13 : Didn't get it

anmol.jindal13 : What does the histogram signify? 

gurupunskill : kaushiksk: Because they're probably random occurrences that does not affect the general behavior of a program?

kaushiksk : Say you're recording temperature on 5 consecutive days. It's 23,25,22,23,98

vilas : <@U59T4ACNQ|vilas> has joined the channel

kaushiksk : The last one because someone put it in a hot tea cup while you were away. Now if you were to say I'll take the mean of all my values and use that as a way to understand my data, you'd be tricked. Because mean will give a lot of preference to 98, which is a value you would otherwise discard

gurupunskill : Like ml is used to predict the nature of something right? The random outlier is probably a rogue occurrence yes?

anmol.jindal13 : Yeah okay. So how do i interpret it by this graph? 

kaushiksk : Mean without 98 - 23.25
Mean with 98 - 38.2

Median would still be in the twenties in both cases.

kaushiksk : So in a histogram if you were to plot this. You will have one long bar in the 20-30 range. Nothing from 30-90. Then a sudden peak from 90-100.

kaushiksk : That's an outlier. Lie outside. Simple as that. Clear ?

anmol.jindal13 : Yes

kaushiksk : Check the thread above if you had any doubts. Tried to clear what <@U586X7GLR>  came up with.

kaushiksk : And this is where you'll learn most. Bring up most

kaushiksk : Now bonus question for browny points.

kaushiksk : data = [255, 168, 125, 11102, 59, 120, 500, 2, 98, 25]

kaushiksk : As we go on, we'll see that this is not exactly how we use ML. If you train your model on Rare occurences, You're missing out on the basic thing youre trying to predict. Normal Values.

kaushiksk : What if

kaushiksk : data = [255, 168, 125, 11102, 59, 120, 500, 2, x, 25]

kaushiksk : Continuing on our previous discusiion

gurupunskill : Cool :+1:

kaushiksk : What value would you replace the missing value with ?

anmol.jindal13 : Mean of all the values excluding11102? 

decisiontreehugger : <@U5872KL73> Outliers can normally really horribly mess with the model training. Whether to drop them or not depends on the dataset and the algorithm you are using. Some algorithms are really robust and can deal with outliers easily, most can't.

shashankp : kaushiksk: median?

kaushiksk : And this is something you will come across very often. Consider a dataset with 3 columns. Age, Income, Gender. Every other column is completely filled. Except for this one cell. We dont want to discard that entire entry. So we try to replace x with the most suitable value we can

gurupunskill : Median

kaushiksk : What do we choose ?

kaushiksk : anmol.jindal13: What if i have 111102 data points and 32 outliers ? Wont it be too much work to cherry pick them all? How do you choose what to pick and what not to ?

samyak : Median

harsha_1810 : median

gurupunskill : Wow xD

kaushiksk : We could do that. But that just works here. Removing one big value is fine.  When you have a data set in the count of thousands having 30 outliers is a possibility. We cant sit and pick each one

kaushiksk : Awesome 1

kaushiksk : We replace x with the median of the dataset. Remember this when we have a session on data cleaning later. Browny points to everyone who answered right!

kaushiksk : Anyone wants to tell us why ? Obligatory.

gurupunskill : Why not mode because, that would be the most probable income yes? We can choose the mode in a specific range? But then how would we choose the range?

gurupunskill : Median makes the most sense because it isn't biased to outliers

gurupunskill : But neither is mode and hence my previous question

kaushiksk : Yes. Median makes most sense because it isnt biased to outliers. We can safely assign a missing value the value of median and it wont drastically effect the entire data.

kaushiksk : Alright

swapnil8424 : Sorry for coming in late. I have read all the messages of the session and have got some idea of the stuff discussed. I will make sure that I don't miss any further session

kaushiksk : If you guys have understood so much, we're more than good to get started.

kaushiksk : swapnil8424: No problem. You can check the logs later. These messages will stay here. We'll move onto python now. So you're right on time

kaushiksk : Alright. Time to code. Time for Python !

decisiontreehugger : I'd just like to add a disclaimer here: There are a variety of ways to deal with holes in the data. While in some cases its a good idea to fill it up with a measure of central tendency(median, mean or mode in that order). There are several cases where it'd probably be better to drop that observation or if there are too many holes in the column, dropping the column itself.

kaushiksk : <@U56N7T2F8|kaushiksk> shared a file: <https://mlieeesmp.slack.com/files/kaushiksk/F59P0HWEQ/mean.py|Mean>

decisiontreehugger : We'll get more in depth on that when applying ML

kaushiksk : Everyone go through that piece of code.

kaushiksk : I'll assume none of you have started off learning python and proceed. Giving only basic info. I'll let yopu explore on your own and give resources. Cool ?

kaushiksk : We'll try drawing analogy with C since you all already know that language.

kaushiksk : That right there is an executable python code. 
You can save it as mean.py or whatever.py and execute it on a terminal by typing `python mean.py`

kaushiksk : First thing we notice, no obligatory headers. Neat right ? Get right to business.

kaushiksk : Ok. Everyone with me ?

kaushiksk : Give a +1 so i know current count :stuck_out_tongue:

kaushiksk : Ok.

kaushiksk : So yeah. You can just get coding. No headers.

kaushiksk : You can open a python interpreter and start typing in that snippet of code line by line and see the output

kaushiksk : or run the entire code as a .py file

kaushiksk : ok

kaushiksk : we'll go line by line

kaushiksk : Line 1. 
data = [255, 168, 125, 11102, 59, 120, 500, 2, 98, 25]

kaushiksk : Whats the biggest difference from C here ?

ag_1709 : kaushiksk: didn't have to declare data before

kaushiksk : Right <@U58DY3T7A> We didn't declare the variable data before.

svarshini : We did not have to specify data type of the list

harsha_1810 : kaushiksk: there's no declaration of the variable data

kaushiksk : Python variables can just spawn anywhere and everywhere. So you need to be extra careful when you use them so you dont misuse them :stuck_out_tongue:

shashankp : kaushiksk: 1246.4

kaushiksk : <@U58R2AR8C> Right again! We didnt specify a data type for the list.

kaushiksk : Two things here. Python variables dont need to be specified their type.

kaushiksk : i = 0

kaushiksk : i = '0'

kaushiksk : i="Myname"

kaushiksk : Are all valid statements. A variable can be assigned any type of data. No declaration required. No specification of datatypes required.

kaushiksk : But to answer what <@U58R2AR8C> brought up, that there was no data type associated with the list, that's because a python list can hold any type of value

kaushiksk : for e.g data = [23,54, 'c', "Haha"] is a valid list!

kaushiksk : Pretty cool right?

kaushiksk : <@U5872KL73>  Now you'll start seeing why its different from an array

kaushiksk : Also, python list is supposed to be a linked list, a data structure you'll learn next year.

kaushiksk : More on lists on section 3.2 here
<http://www.diveintopython.net/toc/index.html>

gurupunskill : kaushiksk: Okay. But shouldn't an element in a linked list point to the next one? Like how would we access that in pythin?

gurupunskill : python*

kaushiksk : supposed to be *more like a linked list than an array. It's a python basic data type. With properties of its own. Read up.

kaushiksk : <@U56N7T2F8|kaushiksk> shared a file: <https://mlieeesmp.slack.com/files/kaushiksk/F59P0HWEQ/mean.py|Mean>

kaushiksk : Line 4. n is an int that holds the length of the list data

kaushiksk : len() does what it seems it should be doing, giving you the length of whatever variable you supply it with

kaushiksk : Now the for loops

kaushiksk : Anyone with an interpreter open can tell me what typing range(10) outputs ina python interpreter ?

harsha_1810 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]

kaushiksk : yes. `for i in range(10)` is just another way of saying `for i in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]`

kaushiksk : the in operator does one simple thing. Give i the value of each item in the list in order.

kaushiksk : <@U56N7T2F8|kaushiksk> shared a file: <https://mlieeesmp.slack.com/files/kaushiksk/F59KHSK6X/img_20170506_202609.jpg|IMG_20170506_202609.jpg>

kaushiksk : You can try printing i inside the for loop to check what is happening

kaushiksk : That's again an analogy between a C for loop and a python one

kaushiksk : key points:
1. for i in range(n):
the ':' at the end . Very important.

2. No braces.
Everything inside the for loop should be indented. All the lines in the same indentation level are like them being inside a flower bracket in C.

kaushiksk : Go through that slide lkater. Shows different parameters you can pass to range() and how it works.

kaushiksk : Any doubts till here? It's very important you understand these simple things clearly

anmol.jindal13 : Nope

kaushiksk : Line 6 is just what it means in C.

kaushiksk : Obligatory thumbs up. I want to see how many of you are here :stuck_out_tongue:

kaushiksk : <@U56N7T2F8|kaushiksk> shared a file: <https://mlieeesmp.slack.com/files/kaushiksk/F59P0HWEQ/mean.py|Mean>

harsha_1810 : float

ag_1709 : Float

samyak : Float

anmol.jindal13 : Floar

kaushiksk : Check comments.

anmol.jindal13 : Float*

samyak : int now prev float

kaushiksk : But i could still avoid it

kaushiksk : meanvalue = float(sum)/n

kaushiksk : would do it

kaushiksk : That's how you do explicit type conversion

decisiontreehugger : Anyone here has python 3?

samyak : Basically float is truncated to int right

kaushiksk : Yes this ^

harsha_1810 : yea got it

shashankp : decisiontreehugger: yes

ayush.work113 : decisiontreehugger: i do

kaushiksk : Anyone has any doubts with that snippet of code now ?

kaushiksk : I hope it all makes sense ?

decisiontreehugger : try dividing 5/4 and tell me what you get

ayush.work113 : 1.25

shashankp : decisiontreehugger: 1.25

decisiontreehugger : Anyone with python 2?

harsha_1810 : python 2 gives 1

decisiontreehugger : Right :slightly_smiling_face:

gurupunskill : I see your point xD

decisiontreehugger : Just remember

ayush.work113 : cool

kaushiksk : This is where the python interpreter comes in handy

kaushiksk : 'When yoiu have a doubt. TRY IT OUT

kaushiksk : Few lines of code anyway.

kaushiksk : That's how you'll learn python :smile:

kaushiksk : Before I proceed. Any doubts, bring them up now. Be absolutely clear with everything till now.

decisiontreehugger : Incase people didn't read the thread what kaushik says stands for python 2 not python 3 you do not have to cast int to float in python 3 during division

decisiontreehugger : This one

kaushiksk : i.e 
Python 2 : 5/4 = 1
Python 3: 5/4 = 1.25

kaushiksk : ok I'll take that as all doubts cleared then ?

kaushiksk : You can pm us or discuss later in the <#C59GM664E|python> channel later on. Or google group. But discuss. Early days you'll be exploring loads of cool stuff in python.

kaushiksk : <@U56N7T2F8|kaushiksk> shared a file: <https://mlieeesmp.slack.com/files/kaushiksk/F59MLG5M0/mean_2.py|Mean 2>

kaushiksk : Now you'll see why i made you print out the output of the range(). Do you guys observe how i takes the value of each entry in the list in every iteration of the loop ?

kaushiksk : This makes python code very very reader friendly. Because I can now write
```
for value in data:
    sum += value
```

kaushiksk : And you know whats happening in your loop. Try to make sure that when you write python code someone can just read it and understand what youre doing :slightly_smiling_face:

kaushiksk : And as for all the print statemjents, you can choose to ignore them now. I included them for completion sake. Print formatting here : <https://pyformat.info/>

kaushiksk : Compare the two versions of the code. The second is the norm that is called "pythonic". Neat, without clutter and easy to read.

kaushiksk : Any reactions? :stuck_out_tongue:

gurupunskill : kaushiksk: I'm surprised python is used as an adjective

kaushiksk : Is the code clear guys ? 
So forget indexing and say hello to the "in" operator :smile: Cause that's how we'll roll now :stuck_out_tongue:

kaushiksk : It's 8:15. I wanted to cover dictionaries but i guess youre better off reading on your own. They're handy data structures. Basically key:value pairs

kaushiksk : You can index lists as data[2], data[0] like you do with arrays

kaushiksk : with dictionaries you access values by keys

kaushiksk : Example

gurupunskill : <@U58R173NY> 6:45 xD

kaushiksk : data = {"kaushiksk" : "Kaushik",
 "decisiontreehugger":"Ambareesh", 
"arvindsaik":Arvind"}

kaushiksk : sorry

kaushiksk : data = {"kaushiksk" : "Kaushik",
"decisiontreehugger":"Ambareesh",
"arvindsaik":"Arvind"}

kaushiksk : If i want the ouput to be "Arvind" how should i access that data item ?

ayush.work113 : data["arvindsaik"]

kaushiksk : Right. That should give you a basic idea.

anmol.jindal13 : kaushiksk: what does the colon represent here? 

shashankp : kaushiksk: data[arvindsaik]

kaushiksk : And you can a list of dictionaires. Lists within dictionaries. Any combination you like

kaushiksk : <http://www.diveintopython.net/toc/index.html>

kaushiksk : Go through chapter 3 here.

kaushiksk : Explains lists and dictionaries and basic data types

kaushiksk : Thats Assignment 1.

kaushiksk : \:p

kaushiksk : But then no one other than you can grade that one :stuck_out_tongue:

kaushiksk : So. We covered different types of data. Mean, Median , Mode, Histograms, Python Basics. Looping, variables, and basic scripting etiquette.

kaushiksk : <https://kadira.io/blog/other/mean-histogram-and-percentiles>

kaushiksk : Hope that was fun and you learnt something new.

kaushiksk : Any doubts ?

gurupunskill : <@U5872KL73|gurupunskill> pinned a message to this channel.

gurupunskill : <@U5872KL73|gurupunskill> pinned a message to this channel.

gurupunskill : <@U5872KL73|gurupunskill> pinned a message to this channel.

kaushiksk : Oh one final thing!

kaushiksk : functions!!

kaushiksk : heres how you define functions in python

kaushiksk : <@U56N7T2F8|kaushiksk> uploaded a file: <https://mlieeesmp.slack.com/files/kaushiksk/F5997P80Z/mean_function.py|Mean function> and commented: Again indentation = flower brackets. No specifying data types. No specifying parameter type.

kaushiksk : def keyword is important.

kaushiksk : <http://www.py4inf.com/>

kaushiksk : Chapter 4 here. That video should suffice.

kaushiksk : Are we all sane ?

kaushiksk : Or does anyone want to get the hell out ? :stuck_out_tongue:

decisiontreehugger : The next session will be introduction to ML we'll also try to cover a little bit up of ML theory if we can. We'll have it either tomorrow or day after so let me know when you want it.

decisiontreehugger : You can tell me now guys :slightly_smiling_face:

gurupunskill : in <#C5617RDN1|general>

kaushiksk : <@U56N7T2F8|kaushiksk> shared a file: <https://mlieeesmp.slack.com/files/kaushiksk/F5AGQFL23/histograms.png|histograms.png>

kaushiksk : I hope the question and the ouput are clear enough to tell you what exactly you have to do yeah ?

kaushiksk : Do this by the end of this week. It'll help us know that you've learnt your basics right as well.

gurupunskill : <@U5872KL73|gurupunskill> pinned a file comment to this channel.

gurupunskill : <@U5872KL73|gurupunskill> pinned <@U56N7T2F8|kaushiksk>â€™s Image <https://mlieeesmp.slack.com/files/kaushiksk/F5AGQFL23/histograms.png|histograms.png> to this channel.

kaushiksk : If you're having trouble, discuss!!

kaushiksk : Google

kaushiksk : Learn :stuck_out_tongue:

kaushiksk : And have fun

kaushiksk : So I guess that's about it for today. Hope you guys had fun! Try to do the Assignments. It's for you learn that's all.

kaushiksk : If you guys have any feedback about the session, positive or negative please do let us know. :slightly_smiling_face:

ag_1709 : kaushiksk: A very well organised session.

kaushiksk : <@U56N7T2F8|kaushiksk> shared a file: <https://mlieeesmp.slack.com/files/kaushiksk/F58LVC35E/Session_1_links|Session 1 links>

anmol.jindal13 : kaushiksk: ?

kaushiksk : That's it for today guys. Any guidelines will be posted in <#C5617RDN1|general>. I'll mail the important links in the google group.

kaushiksk : Check the Post. "Session 1 Links"

kaushiksk : <@U56N7T2F8|kaushiksk> pinned a message to this channel.

kaushiksk : All the important messages have been pinned. All the files shared can also be accessed in the 'View Channel Details' tab.

kaushiksk : That's it for today then. Hope you guys learnt something! Take your time getting the basics right. And ask doubts and discuss! See you during the next session!

mahirjain25 : For assignment 2, do we need to take user input for the list or we can directly take those values ?

kaushiksk : Just take them. 

ckeshava : <@U5A1FHK53|ckeshava> has joined the channel

kaushiksk : <@U58NYR47L> data["arvindsaik"] actually :)

kaushiksk : <@U586X7GLR> the colon separates the keys and values. Keys are to the left of the colon. These are used to access your values. Values are any data you would want to store. 

aditya_a : <@U5A3SFLP3|aditya_a> has joined the channel

ckeshava : <@U5A1FHK53|ckeshava> set the channel purpose: This channel is where all the sessions will be held.

decisiontreehugger : Hi guys

decisiontreehugger : So today's session will be taken by me and <@U58GN7DDX>

decisiontreehugger : We'll be taking a look at what machine learning is

decisiontreehugger : and hopefully cover simple linear regression as well

arvindsaik : show of hands ..those who are present

sanathk : :+1::skin-tone-2:

ayush.work113 : :hand: 

harsha_1810 : :+1:

samyak : :+1: 

gurupunskill : Guys umm just react no pls

ckeshava : :+1:

mahirjain25 : :+1:

darshandv10 : :hand: 

svarshini : :+1:

decisiontreehugger : I think we have about 50% if not more so lets begin

decisiontreehugger : Machine learning is a type of artificial intelligence that aims at giving "computers the ability to learn without being explicitly programmed"

decisiontreehugger : We have three major types of machine learning

decisiontreehugger : <@U58R173NY|decisiontreehugger> uploaded a file: <https://mlieeesmp.slack.com/files/decisiontreehugger/F5BHG5ZST/screen_shot_2017-05-09_at_3.39.34_pm.png|Three Types of Machine Learning>

decisiontreehugger : In supervised learning we are given a bunch of input and output pairs and our goal is to approximate a function f that maps the input to the output

decisiontreehugger : In short find an f so that y = f(x)

decisiontreehugger : In unsupervised learning we have to find a function that in some way describes the data X

decisiontreehugger : In short find an f to describe X

decisiontreehugger : There is still no proper definition of unsupervised learning and you will see why when you look through the different kinds of unsupervised learning algorithms. I've given you probably the least triggering definition of unsupervised learning.

decisiontreehugger : In reinforcement learning we are given a bunch of input,reward pairs(X and Z) and we are to get the system to output y given it's current state

decisiontreehugger : We will mostly be focusing on supervised learning in this SMP and a bit of unsupervised learning and we will not be touching reinforcement learning. We will however give you links on the topic if you are interested.

decisiontreehugger : Cool so far?

gurupunskill : decisiontreehugger: I don't understand this. What are reward pairs? 

decisiontreehugger : Z is basically a reward function

decisiontreehugger : X describes your current state

sanathk : what is that??

gurupunskill : What does a reward function do?

sanathk : reward function 

decisiontreehugger : I'll give you links on rl later if you guys are interested but that'll take a session on it's own

decisiontreehugger : Supervised Learning can again be split further into several topics but to keep it simple I will only be talking about regression and classification

gurupunskill : Okay xD

decisiontreehugger : We use regression techniques when the output we want to predict is a continuos value

decisiontreehugger : We use classification techniques when the output we want to predict is  a discrete value

decisiontreehugger : Here are a few examples of machine learning problems I want you to tell me the possible input features that can be used, what the output will be and whether it's classification or regression

decisiontreehugger : Predicting the value of a house

decisiontreehugger : Checking for a disease

mahirjain25 : decisiontreehugger: regression

ag_1709 : Continuous

decisiontreehugger : Recognising hand written digits

decisiontreehugger : what do you think are the input features we can take?

ag_1709 : Soory regression

ag_1709 : Sorry*

mahirjain25 : decisiontreehugger: classification

ag_1709 : Range of prices

ckeshava : area of the plot

shashankp : Inputs can be area, no. of bedrooms, location

decisiontreehugger : What do you think are the possible input features we can take for this problem?

mahirjain25 : age of house, that is how old it is

ag_1709 : decisiontreehugger: classification

decisiontreehugger : good good

decisiontreehugger : good what do you think will be the input features?

mahirjain25 : simple binary , like whether disease is present or not

ag_1709 : Whether someone suffers from it or not

shashankp : Body temperature

decisiontreehugger : Good <@U58NYR47L>

gurupunskill : Symptoms

harsha_1810 : symptoms

decisiontreehugger : Right

decisiontreehugger : Any Ideas?

gurupunskill : Curves? Irregularities to standard digits?

gurupunskill : Basically accept a small curve and trying to match it with a number.

decisiontreehugger : <@U5872KL73> how would you calculate these things?

shashankp : Darkness of each pixel

decisiontreehugger : <@U58NYR47L> right

decisiontreehugger : We can give the image as an input

kaushiksk : <@U5862FK17> as you'll see later, classification need not be binary, it could so be that given a list of symptoms, age , gender we might have to classify what is the most probable disease he/she might have among disease A,B,C,D etc

decisiontreehugger : Having all the pixel intensities as the features

decisiontreehugger : So is everyone quite comfortable with problem formulation?

mahirjain25 : What if the pixel intensity is continuously varying?

decisiontreehugger : Alright now I'll move on to the basic building blocks of a supervised learning problem

decisiontreehugger : Any model will make use of 3 basic things

decisiontreehugger : *Loss Function* or Error Function or Objective Function or whatever alias this thing has

mahirjain25 : Like if it progressively gets darker or something?

gurupunskill : Yeah but each pixel will still have only one intensity

decisiontreehugger : <@U5862FK17> imagine an image of a hand written digit

decisiontreehugger : Optimization Method

gurupunskill : A pixel is the smallest possible unit in an inage. It can hold only one RGB value...

decisiontreehugger : *The Hypothesis Function*

mahirjain25 : ok..

decisiontreehugger : You need a way to tell how good your model is doing or in other words how well your function f is mapping input values to output values

decisiontreehugger : This is often going to be a function of the distance between the output your model predicted and the actual output it should have given

decisiontreehugger : You can imagine yourself trying to find the best fit line for any generic experiment you might have done in the past

decisiontreehugger : You try to minimise the distance between the line and the points

decisiontreehugger : Is the definition of a loss function clear? Because this is pretty important!

decisiontreehugger : Arvind will go on to explain a loss function soon

decisiontreehugger : So if you want examples don't worry

decisiontreehugger : Next up is the hypothesis function

kaushiksk : Just starting a thread so someone might say something :stuck_out_tongue:

decisiontreehugger : This is basically what your f is going to look like

decisiontreehugger : Btw f is the hypothesis function

darshandv10 : Yeah. So basically what type of curve are we going to work on? Is it just a line or any other curve?

decisiontreehugger : First we will take a look at GLMs

darshandv10 : decisiontreehugger:  Cool

decisiontreehugger : Your f is going to be parameterised by some values we will keep them in a vector and call them W for now

decisiontreehugger : This is where most models vary

decisiontreehugger : It is in their choice of a hypothesis function

decisiontreehugger : We will take a look at the hypothesis function of a glm soon

decisiontreehugger : Is it clear so far?

decisiontreehugger : Finally you need a way to *OPTIMIZE* your *MODEL* to minimise *LOSS*

decisiontreehugger : Your model will be parameterised by W. The question is for what combination of values of W will we get the minimal loss

decisiontreehugger : There are a bunch of different optimisation methods

decisiontreehugger : But for today <@U58GN7DDX> will be covering the steepest descent method

decisiontreehugger : Is this concept making some sense?

decisiontreehugger : Next up I'll take a look at the GLM's hypothesis function

decisiontreehugger : And hopefully it'll become more clear

harsha_1810 : decisiontreehugger: Loss of what?

decisiontreehugger : The loss function that is a measure of how well your model is doing

mahirjain25 : so basically minimise the error between actual and hypothetical right?

gurupunskill : Yep

decisiontreehugger : So the hypothesis function of a GLM generally looks like this I say generally because there are often slight variations

decisiontreehugger : f(X;W) = w1x1 + w2x2 + ...... + wnxn + w0

decisiontreehugger : Now for a single input variable it'll look like f(X;W) = w1x1 + w0

decisiontreehugger : Which is nothing but y = mx + c

gurupunskill : decisiontreehugger: What is a GLM

mahirjain25 : general linear model

decisiontreehugger : Generalized Linear Model

decisiontreehugger : So you can kind of translate it to 2D space

decisiontreehugger : So we want our *OPTIMIZATION* method to find us the best W for our *MODEL* so that it can best minimize the *LOSS*

decisiontreehugger : I hope this above concept is clear for everyone because all models come back to this again and again

sanathk : <@U58R173NY> the loss function and hypothesis function are of same degree..?

kaushiksk : Here x1,x2,x3,x4 ... xn are the input values. w1,w2,w3, ... wn are weights we assign to each input so they predict the right ouput.

decisiontreehugger : Be it Neural Nets Decision Trees or SVMs

ag_1709 : what decides the values contained in W?

gurupunskill : Okay so we simulate different values of w1, w2 .. etc to find our function right?

decisiontreehugger : Your loss function looks at how well X is being mapped to y. Your hypothesis maps X to y

darshandv10 : decisiontreehugger: SVM??

gurupunskill : We simulate different values of W to get our f(x)

gurupunskill : ^ <@U58DY3T7A>

decisiontreehugger : <@U58DY3T7A> that's the goal we need to find the best W values guided by our purpose to minimize Loss function

decisiontreehugger : Support Vector Machines they're another ML algorithm

decisiontreehugger : Is the idea clear so far?

decisiontreehugger : Please do ask questions if still not clear

decisiontreehugger : Alright now over to <@U58GN7DDX> he'll give an explanation of a GLM that we'll get you guys to code soon

gurupunskill : The loss function is basically L = f(x,w)-y. Objective is to make L -&gt; 0 for all x. The degree should be the same <@U59JHRLAJ>

arvindsaik : hi guys

arvindsaik : ok so now let us say our hypothesis is h(x)

arvindsaik : We have a function h(x) which is trying to predict the value y given x.

arvindsaik : So how do you think we can make h(x) as close to y(actual value in dataset)?

arvindsaik : because if we achieve this our model is ready for prediction

arvindsaik : guys ?
So how do you think we can make h(x) as close to y(actual value in dataset)?

kaushiksk : Anyone who still isn't clear on what the hypothesis and loss functions are head over here and put up your doubts.

arvindsaik : ok first off did you guys understand hypothesis function and what it signifies ?

shashankp : arvindsaik: changing the values of co-efficients(w1,w0) after each iteration to minimise error

ckeshava : can we take the mod of differences between the predicted and actual values, and then try to reduce it ?

decisiontreehugger : <@U58NYR47L> nice

arvindsaik : thats correct but diving into detail what are u achieving by chnaging weights?

kaushiksk : <@U5872KL73> they need not be of the same degree.. We'll discuss this further. For now listen to what arvind is teaching.

arvindsaik : *changing

gurupunskill : Okay xD

arvindsaik : thats a good idea ..

arvindsaik : mod is essentially root(square(x))

arvindsaik : so we will be taking x^2

decisiontreehugger : ckeshava: You can! It's one way this is called Mean Absolute Error however it isn't appealing for most optimisation methods since it's not easily differentiable.

shashankp : arvindsaik: by changing weights hypothesis function h(x) is made closer to y

arvindsaik : correct !
we are reducing the error !

ckeshava : oh okay

arvindsaik : am i going too fast ?

ckeshava : oh ok, thanks for thanks for the info :grinning:

arvindsaik : i see only 2 of u replying so..

gurupunskill : L(x,w) = f(x,w) - y(x) 
Okay so I guess it's dependent on y(x) as well? I don't think the degree matters at all. Knowing the degree for L wouldn't help you in any way would it?

mahirjain25 : so we take square of the error ?

ayush.work113 : arvindsaik: I think it's fine.

arvindsaik : yup

gurupunskill : I went to get something to eat xD

darshandv10 : arvindsaik: We are following.:v:

arvindsaik : so if we say the hypothesis is

arvindsaik : <@U58GN7DDX|arvindsaik> uploaded a file: <https://mlieeesmp.slack.com/files/arvindsaik/F5A6WRXUH/pasted_image_at_2017_05_09_07_19_pm.png|Pasted image at 2017-05-09, 7:19 PM>

decisiontreehugger : Here the thetas are basically w

arvindsaik : we are trying to  minimise the cost function which is basically square of error

arvindsaik : <@U58GN7DDX|arvindsaik> uploaded a file: <https://mlieeesmp.slack.com/files/arvindsaik/F5APLGJ1G/pasted_image_at_2017_05_09_07_20_pm.png|Pasted image at 2017-05-09, 7:20 PM>

arvindsaik : where m is the number of datapoints on the graph or no. of training examples

arvindsaik : This approach is called linear regression

arvindsaik : formally

arvindsaik : linear regression is an approach for modeling the relationship between a scalar dependent variable y and one or more explanatory variables (or independent variables) denoted X.

arvindsaik : all clear ?

samyak : Loss function maps x to y??

arvindsaik : hypothesis maps x to y

decisiontreehugger : samyak: Thats the hypothesis

decisiontreehugger : Loss tells you how well hypothesis is doing

svarshini : Okay so, 
The hypothesis function is basically the one which maps the given input values to the output y
The loss function gives the amount of deviation of the predicted output from the actual output
The optimisation method chosen helps to minimise the loss function
Did I get this right?

samyak : Ok so error square and its summation gives u loss function and hypothesis function maps it

arvindsaik : carry on ?

decisiontreehugger : <@U58R173NY|decisiontreehugger> uploaded a file: <https://mlieeesmp.slack.com/files/decisiontreehugger/F5A434Z40/reg_error.gif|reg_error.gif>

decisiontreehugger : For anyone still in doubt

kaushiksk : The 2 in the denominator is for mere convenience as you will see when we get to gradient descent.

decisiontreehugger : black line is hypothesis

decisiontreehugger : Loss is sum of distance marked in pretty colors

arvindsaik : <@U58GN7DDX|arvindsaik> uploaded a file: <https://mlieeesmp.slack.com/files/arvindsaik/F5APRRGCS/pasted_image_at_2017_05_09_07_28_pm.png|Pasted image at 2017-05-09, 7:28 PM>

arvindsaik : coming back to h(x) here we have assumed only one feature which we think y depends on

arvindsaik : what if there are multiple features

arvindsaik : what if cost of a flat depended not only on square feet

arvindsaik : but also no. of bedrroms etc .. ?

kaushiksk : <@U58R2AR8C> absolutely.

arvindsaik : with one variable method is called : simple linear regression

arvindsaik : with multiple variables : multiple linear regression

darshandv10 : arvindsaik: Maybe we can use y= w1x + w2x +w3  I guess

gurupunskill : Yeah

arvindsaik : correct

gurupunskill : so f(x,w0,w1,w2)

arvindsaik : refer to hypothesis in <@U58337ESC> 's comment

arvindsaik : one method to get the right hypothesis from loss/cost function is gradient descent

arvindsaik : which i will be doing now..

arvindsaik : proceed ?

kaushiksk : <@U5872KL73> the thing you need to realize is, the values y need not come from a polynomial function. Or from any function for that matter. Take housing prices. Do they obey a formula? They may not in general. These are just discrete values collected from a survey, say. So what we are trying to do with our hypothesis function is *FIT* a function that will actually help us map our input to the given ouput and see if there actually is a relationship between the two :slightly_smiling_face:

arvindsaik : ok so now given the cost function

arvindsaik : actually given any function how would u minimise it

kaushiksk : For those still in doubt.

gurupunskill : I understand. 

arvindsaik : as you all probably learnt in 12th grade

arvindsaik : we differentiate it get to know the slope at that point and try moving closer and closer to slope 0

arvindsaik : which will be the minima

arvindsaik : clear ?

arvindsaik : what do we differentiate wrt ?

arvindsaik : we do it wrt to theta

arvindsaik : as we need to find the right weights

darshandv10 : arvindsaik: Slope 0 is a constant function right. What if our data doesn't cluster around this line?

arvindsaik : we are not clustering data around this line
the slope 0 is just to make sure we are at the lowest possible value of the cost function 
thereby helping us to get a good hypothesis

darshandv10 : arvindsaik: Why do we move towards slope 0 ? We need to get the slope fit to the minimum difference right?

arvindsaik : we keep changing theta till the cost function comes to its minimum

kaushiksk : Ok. So let's make it clear where the cost function and gradient descent fits in here.

kaushiksk : Read what <@U58GN7DDX> had said till now and relate it to what I'm going to say.

kaushiksk : <@U58337ESC> move on to main thread

arvindsaik : slope doesnt mean anything to us it is just helping us find the minimum cost 
i am not quite sure if u are visualising it properly

kaushiksk : Ok. So i did a survey and got Prices of 100 houses.

kaushiksk : I got some additional info. Height width and Age of the house

kaushiksk : So x1 - height

kaushiksk : x2- width
x3 - age

kaushiksk : y - price

kaushiksk : hold on to that.

arvindsaik : how do we implement the change of theta so that cost function reaches least value

arvindsaik : <@U58GN7DDX|arvindsaik> uploaded a file: <https://mlieeesmp.slack.com/files/arvindsaik/F5AMYS4BV/pasted_image_at_2017_05_09_07_45_pm.png|Pasted image at 2017-05-09, 7:45 PM>

arvindsaik : basically for each weight we are moving closer to minimum 
and once differential reaches 0 we have trained our weights to reduce the cost/loss function

arvindsaik : clear ?

darshandv10 : arvindsaik: Oh Ok now I understand better.

arvindsaik : here alpha is called the learning rate 
the higher it is faster the learning 
but there might be problems with very high alpha

arvindsaik : so we need to choose an optimum value for alpha

arvindsaik : good :slightly_smiling_face:

arvindsaik : and for those of u interested in the math of differential here goes

arvindsaik : <@U58GN7DDX|arvindsaik> uploaded a file: <https://mlieeesmp.slack.com/files/arvindsaik/F5ASZC7PF/pasted_image_at_2017_05_09_07_48_pm.png|Pasted image at 2017-05-09, 7:48 PM>

arvindsaik : I know it might be a little over board but feel free to ask doubts and refer to the links which i will be posting

arvindsaik : over to kaushik

kaushiksk : Ok. So I know that all of you understand each of the parts to an extent but are waiting for it to make sense.

kaushiksk : Everybody still holding on to the hypothetical data i collected ? 
100 examples of x1,x2,x3 y .

kaushiksk : Show of hands please.

kaushiksk : Cool now i have the data.

kaushiksk : So I get this idea that maybe the price y of a house is dependent linearly on the age of the house.

kaushiksk : What do you think my hypothesis will look like ?

ag_1709 : =w1x1+x0 (if age is the only parameter)

ayush.work113 : kaushiksk: a function of x3

kaushiksk : x1 - Height
x2 - weight
x3 - Age

ag_1709 : w0*

kaushiksk : Gimme h(x,w)

gurupunskill : h = w0 + w1x1+ w2x2 + w3x3

kaushiksk : Here I'll only be using age. So i'll call this x for convenience.

mahirjain25 : = w0 + x1w1 +x2w2 + x3w3

ag_1709 : h(x,w)= w3x3+w2x2+w1x1+w0

harsha_1810 : =w0+x1w1+x2w2+x3w3

kaushiksk : What made you change your answer <@U58DY3T7A> ?

kaushiksk : "So I get this idea that maybe the price y of a house is dependent linearly on the age of the house."


Which is another way of saying I dont want anything to do with height or width here :stuck_out_tongue:.


So now, what is h(x,w) ?

ag_1709 : because age wasnt the only factor influencing the cost, there were other parameters too

piyush : Same as what <@U58DY3T7A> said before

gurupunskill : h = w0 + xw1

mahirjain25 : ^

kaushiksk : Right. You'll see what I'm doing in a moment.

ag_1709 : if only age is considered, I stand by my previous answer

kaushiksk : So I take all the 100 values of age and represented by x

kaushiksk : So my hypothesis is h(x,w) = w0 + w1x

kaushiksk : Which just another way of saying "I think there is a function h(x) which has weights w1,w0 (which we shall find) which i think will help me give exactly those values in y for each example

kaushiksk : I don't know what w1,w0 are. I'll find them later. Right now i'm just making a hypothesis that there actually is some sort of linear relationship between Age x and price y and this relationship will be defined by w1,w0

kaushiksk : So that's what your weights are. They define the relationship between two variables. How good your weights are, will depict the strength of your model, i.e given any example x(i), how close is h(x(i)) to y.

kaushiksk : If it's very close, my job is done :slightly_smiling_face:

kaushiksk : Cool ?

kaushiksk : So take a deep breathand read that again. :stuck_out_tongue:

kaushiksk : Ok so now, I'll just assume random values for w1,w0 
I'll say w1 = 0.5, w0 = 0.4 for no apparent reason.

decisiontreehugger : And always remember the basic to any ML model *OPTIMIZE* the vector W which parameterises the *HYPOTHESIS* to minimize *LOSS*

kaushiksk : ^ We'll see that in action now. And this is very very important. Again, I hope you remember what <@U58GN7DDX> has taught you today.

kaushiksk : Right so with those weights, my hypothesis is now h(x) = 0.4 + 0.5x

kaushiksk : I'm way too confident about my guesses so i declare that gimme any value of x (Age) I'll tell you the price for y.

kaushiksk : Do you think Im doing a wise thing ?

kaushiksk : Fine.

kaushiksk : So how will you tell me i'm not doing a wise thing ?

shashankp : kaushiksk: calculate the error

kaushiksk : How will you prove it to me mathematically ?

gurupunskill : Show that your variance is high

darshandv10 : kaushiksk: The cost function must be minimum which doesn't happen by a random selection.

svarshini : kaushiksk: By calculating the loss function?

gurupunskill : Basically yeah ^

ckeshava : try choosing another pair of w0 and w1 such that the loss is lesser in the latter case ?

kaushiksk : You calculate the error. You take the 100 examples you have. For ever x(i) you find h(x(i), compare that with corresponding y, find the error.

decisiontreehugger : <@U5872KL73> Loss is not your variance

kaushiksk : Which is again just another way of saying, evaluate the loss function :slightly_smiling_face:

decisiontreehugger : <@U5872KL73> you will understand when we actually get to bias and variance

gurupunskill : Yeah I geddit xD

kaushiksk : That's what a loss(cost) function is for. To tell you how good your hypothesis is. And since I've already convinced you that your hypotheis is completely dependent on your weights, the loss function measures how good your weights are.

kaushiksk : Cool ?

decisiontreehugger : <@U5872KL73> You can have 0 loss on your training data by fitting a polynomial of n+1 degree with n being your total number of data points resulting in a ridiculously high variance hypothesis function

kaushiksk : Naice.

kaushiksk : So now we see things falling into place.

kaushiksk : But wait.

kaushiksk : Why do we need something like gradient descent ?

kaushiksk : The answer is one word long.

kaushiksk : And has been said before :stuck_out_tongue: On the buzzer!

shashankp : kaushiksk: optimize

gurupunskill : Why? How would loss be 0?

piyush : Accuracy

ayush.work113 : Optimisation

svarshini : Optimization

kaushiksk : And the answer IS

kaushiksk : *OPTIMIZATION*

darshandv10 : Optimize

mahirjain25 : optimise

gurupunskill : If loss is 0 doesn't that mean our model predicted everything correctly? I'm assuming the definition of variance is something else entirely?

kaushiksk : I dont want ANY hypothesis. I want THE hypothesis. The one that predicts the right values.

kaushiksk : Which again is another way of saying

kaushiksk : I need h(x) = w0 + w1x

kaushiksk : with values w0,w1

kaushiksk : such that

kaushiksk : when i evaluate the loss function

kaushiksk : I get a very very low value.

kaushiksk : So that is why, *MINIMIZING THE LOSS FUNCTION* is such a big deal.

mahirjain25 : Yeah I'm not clear about how variance would be high in that case?

kaushiksk : *MINIMIZING THE LOSS FUNCTION* = getting right values of w0,w1

kaushiksk : And this is where gradient descent comes in.

kaushiksk : <@U56N7T2F8|kaushiksk> shared a file: <https://mlieeesmp.slack.com/files/arvindsaik/F5AMYS4BV/pasted_image_at_2017_05_09_07_45_pm.png|Pasted image at 2017-05-09, 7:45 PM>

kaushiksk : Notice that here, we change the values of theta. Which is just like saying, "Ok my random values gave me high loss, so i need to shift to some other value"

kaushiksk : Again, notice that in the next step, we are still jumping to two arbitrary values, only difference here is, they are controlled by the loss function ( i.e how well my hypothesis predicts ouput)

kaushiksk : So now it shouldnt be hard to notice that as my loss function reaches zero, I will no longer be updating my weights.

kaushiksk : That is *drumrolls* I have reached my optimum set of weights!

kaushiksk : Ok. I tried my best. That's as far as my understanding helps me dumb it down :joy:

kaushiksk : Now obviously, since I have optimum values, give me any x(i) among the 100, I can confidently say i'll give a near perfect value of y Right ?

kaushiksk : Moral of the story: Never trust a guy who guesses without the right math to back it up.

kaushiksk : Now. Do you know whats the most exciting thing about my hypothesis right now ?

kaushiksk : Anyone ?

kaushiksk : Give me 50 more values for x that you just collected from another survey. Now based on the fact that my hypothesis correctly predicts y values for the 100 examples i trained it on, I can confidently tell you that I can predict y values for these 50 examples without you telling me at all.

kaushiksk : And there we have reached the end of the ML pipeline.

kaushiksk : The 100 values i trained on - Training set.
The 50 new values i test on - Test Set. 
The true awesomeness of your model is actually measured on how well it can predict on inputs it has never seen before, that is, the test set. You'll see a lot of this as we go along.

kaushiksk : For now, is Gradient Descent clear ?

kaushiksk : It's up to you to figure out the math.

kaushiksk : I want you to be clearwith what Hypothesis, Loss Function. Optimization and weights and predicted and actua loutputs mean. And how you go about fitting the right weights.

kaushiksk : After that its just a matter of which loss function or which optimization technique or which ML algorithm for that matter.

kaushiksk : Now. To your wrong answers. I found that the age predicts prices fine. BUt maybe i should include more features. So i include the width and height. Whats my hypothesis now ? :stuck_out_tongue:

kaushiksk : h = w0 + w1x1 + w2x2 + w3x3

kaushiksk : All i need to do now is find the best ws using the same approach i used earlier.

kaushiksk : Clear ?

decisiontreehugger : Alright now for the exciting part

decisiontreehugger : This weeks assignment

decisiontreehugger : You guys have to code a GLM from scratch using numpy

decisiontreehugger : You guys will have to learn numpy on your own(it's pretty simple if you guys have doubts you can post on slack)

decisiontreehugger : <@U58R173NY|decisiontreehugger> uploaded a file: <https://mlieeesmp.slack.com/files/decisiontreehugger/F5AU0GNKF/train_data.csv|train_data.csv>

kaushiksk : This will be a good time to use the channels to post messages like "Did any of you find a good reference for numpy?" or "Did any of you figure out how to multiple matrices in numpy elementwise?"

decisiontreehugger : <@U58R173NY|decisiontreehugger> uploaded a file: <https://mlieeesmp.slack.com/files/decisiontreehugger/F5ASNQ2DS/test_input.csv|test_input.csv>

decisiontreehugger : You will have to train using examples from train data

decisiontreehugger : And predict on test input

gurupunskill : kaushiksk: <http://www.holehouse.org/mlclass/03_Linear_algebra_review.html> Much?

decisiontreehugger : We have the solutions to the test input with us so we'll let you know your errors

decisiontreehugger : lol that's tiny

decisiontreehugger : Also this assignment will be peer reviewed

gurupunskill : Yeah xD I saw a video on why you need to know Linear algebra for ML and I'm like whoa pls no

decisiontreehugger : Each of you will have to review 3 of your fellow SMP mates code

decisiontreehugger : And give a rating from 1-10

decisiontreehugger : Giving reasons why

kaushiksk : This is so much fun.

decisiontreehugger : This assignment will be very very hard

decisiontreehugger : So the faster you get started the better

decisiontreehugger : The time is 1 week

kaushiksk : ALso don't get too competitive. :stuck_out_tongue: Focus on learning together. Implement it your own way :slightly_smiling_face:

decisiontreehugger : We'll let you know which of your peers' assignments you will be grading

decisiontreehugger : Daily updates will have to be posted on the assignment page based on your progress so far

decisiontreehugger : Things you will have to learn: numpy and pandas

arvindsaik : Also if you are still having issues understanding today's session 
Go through videos in Coursera ML course by Andrew ng

decisiontreehugger : If any of you know OOP making the GLM in an object will be a big plus +

decisiontreehugger : If not just do it in functions

decisiontreehugger : Is that clear so far?

kaushiksk : Ok im a nice guy so ill tel you how to start. The first sentence to type on google will be "How to read data from csv file into a numpy array " :stuck_out_tongue:

gurupunskill : kaushiksk: Done xDDDD

decisiontreehugger : Don't expect too much help from us on this assignment we want to test your googling capabilities :stuck_out_tongue:

kaushiksk : Once you're there it's you and your peers. Make very very good use of all the channels. THis will really help us assess how well you can learn among yourself and what pace we have to proceed with.

decisiontreehugger : Again daily posts on assignments page showing your current progress

decisiontreehugger : We really want to be able to see the pace you guys are moving with. Is that clear?

kaushiksk : You can come to us if none of you are able to solve anything. But first make sure you've discussed amongst yourselves.

decisiontreehugger : We'll be looking at the groups :stuck_out_tongue:

decisiontreehugger : Also by showing I mean posting code

kaushiksk : If you're not excited right now you might as well give up on engineering.

decisiontreehugger : I highly suggest the use of jupyter notebooks for this assignment

decisiontreehugger : You can convert your ipynb to a .py file and post it in <#C5A4MD7HN|assignments>

decisiontreehugger : <@U58R173NY|decisiontreehugger> pinned their CSV <https://mlieeesmp.slack.com/files/decisiontreehugger/F5AU0GNKF/train_data.csv|train_data.csv> to this channel.

decisiontreehugger : <@U58R173NY|decisiontreehugger> pinned their CSV <https://mlieeesmp.slack.com/files/decisiontreehugger/F5ASNQ2DS/test_input.csv|test_input.csv> to this channel.

kaushiksk : Or just upload it to <http://gist.github.com|gist.github.com> and send us the link. Github renders then notebooks so its very very convenient.

decisiontreehugger : We want to see all your posts by 9 pm tomorrow

kaushiksk : Like this. <https://gist.github.com/kaushiksk/a1f0a0d6bb2a32c214f6a80d98d5c312>

decisiontreehugger : glhf

decisiontreehugger : That'll be all

decisiontreehugger : With that session 2 is now complete

kaushiksk : Have fun over the week! Hope it was a good session!

kaushiksk : This.

kaushiksk : Show of hands just to make sure you guys still know what show of hands means at the end of this.

ckeshava : :+1:

ckeshava : :joy:

piyush : :+1: 

ckeshava : thanks a lot for the wonderful session !! good night

kaushiksk : We'll be here if you have any doubts on todays session.

